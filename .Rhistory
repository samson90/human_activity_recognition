fit = glmnet(training[1:8], training$CompressiveStrenght)
fit = glmnet(training[1:8], training$CompressiveStrength)
names(training)
library(glmnet)
set.seed(233)
summary(training)
matrix = data.matrix(training[1:8])
library(glmnet)
set.seed(233)
fit = glmnet(matrix[1:8], training$CompressiveStrength)
?glmnet
fit = glmnet(matrix, training$CompressiveStrength)
plot(fit)
par=mfrow(c(1,1))
par(mfrow=c(1,1))
plot(fit)
?plot.enet
plot.enet(fit, xvar=c(penalty))
plot.enet(fit, xvar="penalty")
plot.enet(fit)
fit
summary(fit)
plot.enet(fit, xvar="penalty")
?Lasso
plot.enet(fit, xvar="penalty")
plot.enet(fit)
plot(fit)
plot(fit, fit$Lambda)
plot(fit, fit$Lambda)
plot(fit, fit$lambda)
plot(fit, fit$Lambda)
coef(fit,s=0.1)
coef(fit,s=1)
coef(fit,s=4)
coef(fit,s=8)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
url <- geURL('https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv')
library(RCurl)
url <- geURL('https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv')
url <- getURL('https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv')
library(lubridate)  # For year() function below
dat = read.csv(text=url)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(forecast)
?bats
bats(tstrain)
tsFit <- bats(tstrain)
x.ets <- ets(tstrain)
x.fore <- forecast(x.ets)
x.fore
tsTrain
tstrain
forecast(tstrain)
bats(tstrain)
summary(bats)
tsFit = bats(tstrain)
summary(tsFit)
tsFit$ma.coefficients
testing
forecast(tstrain)
?forecast
forecast(tstrain, h = nrow(testing))
for (i in nrow(testing)){
asdfsadf
}
clear
fc <- forecast(tstrain, h=nrow(testing))
fc
summary(testing)
inrange <- testing$visitsTumblr > fc[,5] & testing$visitsTumbler < fc[,6]
inrange <- testing$visitsTumblr > fc[5] & testing$visitsTumbler < fc[6]
fc
fc$Lo 95
fc$'Lo 95'
fc$Lo_95
fc[,5]
fc[5]
?make.names
names(fc)
fc$lower
fc$lower[2]
fc$lower[,2]
inrange <- testing$visitsTumblr > fc$lower[,2] & testing$visitsTumbler < fc$upper[,2]
inrange
inrange <- (testing$visitsTumblr > fc$lower[,2] & testing$visitsTumbler < fc$upper[,2])
inrange
inrange <- testing$visitsTumblr > fc$lower[,2] && testing$visitsTumbler < fc$upper[,2]
inrange
testing$visitsTumblr
inrange <- testing$visitsTumblr > fc$lower[,2] & testing$visitsTumbler < fc$upper[,2]
inrange <- testing$visitsTumblr > fc$lower[,2]
inrange
greater <- testing$visitsTumblr > fc$lower[,2]
below <- testing$visitsTumblr < tc$upper[,2]
below <- testing$visitsTumblr < fc$upper[,2]
below
sum(below) / nrow(below)
sum(below) / length(below)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library(e1071)
svmModel <- train(training[1:8], training$CompressiveStrength)
svmModel <- svm(training[1:8], training$CompressiveStrength)
predict(svmModel, testing)
predict(svmModel, testing[1:8])
?rmse
library(rmse)
library(RMSE)
library(gydroGOF)
(predict(svmModel, testing[1:8]) - testing$CompressiveStrength) ^ 2
mean((predict(svmModel, testing[1:8]) - testing$CompressiveStrength) ^ 2)
install.packages('hydroGOF')
library(hydroGOF)
set.seed(325)
svmModel <- svm(training[1:8], training$CompressiveStrength)
rmse(predict(svmModel, testing[1:8]), testing$CompressiveStrength)
library(tree)
install.packages(tree)
install.packages('tree')
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats.data.frame(Carseats,High)
Carseats=data.frame(Carseats,High)
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
set.seed(2)
train=sample(1:nrow(Carseats),200)
Carseats.test = Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="Class")
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
prune.carseats=prune.misclass(tree.carseats,best=15)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
library(MASS)
set.seed(1)
train=sample(1:nrow(Boston),nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.Boston)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
Boston$medv
max(Boston$medv)
Boston[205,]
Boston[426,]
library(randomForest)
set.seed(1)
bag.boston = randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag=predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag,boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
bag.boston=randomForest(medv~.data=Boston,subset=train,mtry=13,ntree=25)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
yhat.bag=predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
set.seed(1)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.Boston)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
c <- c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.4, 0.3, 0.2, 0.1, 0)
g <- 0:10 / 10
g
e <- g
g <- g * log(1-g)
g
g <- e
g <- g * (1- g)
g
e <- e * log(e) * -1
e
c <- c(.1, .2, .3, .4, .5, .4, .3, .2, .1, 0)
g <- 1:10 / 10
e <- g
g <- g * (1-g)
e <- e * log(e) * -1
g
e
c
library(ggplo2)
library(ggplot2)
qplot(1:10/10, c)
qplot(1:10/10, c, geom="line")
ggplot(1:10/10)+geom_line(aes(y=c))
df = data.frame(1:10/10, c, g, e)
df
ggplot(df, aes(X1.10.10)) +
geom_line(aes(y = c, colour = "c")) +
geom_line(aes(y = g, colour = "g")) +
geom_line(aes(y = e, colour = "e"))
.1 + .15 + .2 + .2+ .55 +.6 + .6+ .65+ .7 + .75
/10
4.5/10
ncol(Boston)
names(Boston)
library(caret)
library(ISLR)
Boston
names(Boston)
?train
?createDataPartition
inTrain <- createDataPartition(Boston$medv, p=0.7, list=FALSE)
InTrain
inTrain
training <- Boston[inTrain,]
testing <- Boston[-inTrain,]
modelFit <- train(training[1:13], training$medv, method=rf, mtry=c(1,2,4,8,13))
modelFit <- train(training[1:13], training$medv, method='rf', mtry=c(1,2,4,8,13))
modelFit <- train(training[,1:13], training$medv, method='rf', mtry=c(1,2,4,8,13))
grid <-expand.grid(mtry=c(1, 2, 4, 8, 13))
grid
modelFit <- train(training[,1:13], training$medv, method='rf', tuneGrid=grid)
modelFit <- train(training[,1:13], training$medv, method='rf', tuneGrid=grid, ntree=c(10,25,50,100,200,500))
library(randomForest)
modelFit <- randomForest(medv~.,data=Boston,subset=inTrain,mtry=c(1,2,4,8,13),ntree=c(10,25,50,100,200,500))
for (i in c(1,2,4,8,13)){
for (j in c(10,25,50,100,200,500)){
}}
?matrix
matrix(data=1:30,nrow=5, ncol=6)
m <- matrix(data=1:30,nrow=5,ncol=6)
for (i in c(1,2,4,8,13)){
for (j in c(10,25,50,100,200,500)){
names(training)
modelFit <- train(training[,1:13], training$medv, method='rf', tuneGrid=grid)
modelFit
library(hydroGOF)
names(modelFit)
modelFit[1]
summary(Carseats)
nrow(Carseats)
?createDataPartition
inTrain <- createDataPartition(Carseats$Sales, p=0.7, list=FALSE)
training <- Carseats[inTrain,]
testing <- Carseats[-inTrain,]
modelFit <- train(training[2:11],training$Sales,method='rpart')
plot(modelFit)
modelFit
plot(modelFit$finalModel)
text(modelFit$finalModel,pretty=0)
par(mfrow=c(1,1))
plot(modelFit$finalModel)
text(modelFit$finalModel,pretty=0)
plot(modelFit$finalModel)
text(modelFit$finalModel,pretty=0)
names(modelFit)
modelFit$results
2.347328*2
?train
trControl <- trainControl(method='cv')
modelFit <- train(training[2:11],training$Sales,method='rpart',trainControl=trControl)
modelFit
?trainControl
modelFit <- train(training[2:11],training$Sales,method='rpart',trControl=trControl)
modelFit
tree.carseats = tree(Sales~.,data=training)
plot(tree.carseats)
text(tree.carseats,pretty=0)
p <- predict(tree.carseats, testing[,2:11])
mse(p, testing$Sales)
cv.carseats=cv.tree(tree.carseats)
plot(cv.carseats)
prune.carseats = prune.tree(tree.carseats,best=9)
plot(prune.carseats)
mse(predict(prune.carseats, testing[,2:11]), testing$Sales)
bag.carseats = randomForest(Sales~.,data=training,mtry=3, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
?importance
importance(bag.carseats)
bag.carseats = randomForest(Sales~.,data=training,mtry=4, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
bag.carseats = randomForest(Sales~.,data=training,mtry=5, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
bag.carseats = randomForest(Sales~.,data=training,mtry=6, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
bag.carseats = randomForest(Sales~.,data=training,mtry=7, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
bag.carseats = randomForest(Sales~.,data=training,mtry=8, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
bag.carseats = randomForest(Sales~.,data=training,mtry=9, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
bag.carseats = randomForest(Sales~.,data=training,mtry=10, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
bag.carseats = randomForest(Sales~.,data=training,mtry=2, importance=TRUE)
mse(predict(bag.carseats,testing[,2:11]), testing$Sales)
names(OJ)
nrow(OJ)
training <- createDataPartition(OJ$Purchase, p=800/1070, List=FALST)
training <- createDataPartition(OJ$Purchase, p=800/1070, List=FALSE)
training <- createDataPartition(OJ$Purchase, p=800/1070, list=FALSE)
training <- createDataPartition(OJ$Purchase, p=799/1070, list=FALSE)
inTrain <- createDataPartition(OJ$Purchase, p=799/1070, list=FALSE)
training <- OJ[inTrain,]
testing <- OJ[-inTrain,]
treeOJ <- tree(Purchase~.-Buy, data=training)
treeOJ <- tree(Purchase~.-Buy, training)
names(training)
treeOJ <- tree(Purchase~., data=training)
summary(treeOJ)
plot(treeOJ)
text(treeOJ, pretty=0)
treeOJ
OJ$Purchase
?OJ
treeOJ
summary(OJ$LoyalCH)
names(testing)
confusionMatrix(predict(treeOJ, testing[,-1]), testing$Purchase)
predict(treeOJ, testing[-,1])
predict(treeOJ, testing[,-1])
?predict
confusionMatrix(predict(treeOJ, testing[,-1], type="class"), testing$Purchase)
cv.OJ <- cv.tree(treeOJ)
cv.OJ
plot(cv.OJ)
?cv.tree
cv.tree
cv.OJ
cv.OJ <- cv.tree(treeOJ, method='cv')
cv.OJ <- cv.tree(treeOJ, method='misclass')
plot(cv.OJ)
cv.OJ
prune.OJ = prune.tree(treeOJ, best=5)
confusionMatrix(predict(prune.OJ, testing[,-1], type='class'), testing$Purchase)
prune.OJ
summary(prune.OJ)
summary(treeOJ)
confusionMatrix(predict(prune.OJ, testing[,-1], type='class'), testing$Purchase)
confusionMatrix(predict(treeOJ, testing[,-1], type='class'), testing$Purchase)
summary(Hitters)
is.na(Hitters$Salary)
df <- Hitters[is.na(Hitters$Salary),]
nrow(df)
df <- Hitters[!is.na(Hitters$Salary),]
nrow(df)
Hitters$Salary <- log(Hitters$Salary)
training <- Hitters[1:200,]
testing <- Hitters[201:,]
testing <- Hitters[201:322,]
library(gbm)
boost.Hitters=gbm(Salary~.data=training,distribution="gaussian",n.trees=1000)
boost.Hitters=gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000)
training <- df[1:200,]
testing <- Hitters[201:263,]
boost.Hitters=gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000)
?gbm
boost.Hitters <- 1:10
boost.shrinkage <- c(.0001,.0005,.001,.005,.01,.05,.1,.5,1,5)
for (i in 1:10){
boost.Hitters[i] <- gbm(Salary~.data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
for (i in 1:10){
boost.Hitters[i] <- gbm(Salary~.,data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
}
boost.Hitters[1]
boost.Hitters=gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000)
boost.Hitters
boost.Hitters <- 1:10
for (i in 1:10){
boost.Hitters[i] <- gbm(Salary~.,data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
}
boost.shrinkage[1]
boost.Hitters[[1]
]
boost.Hitters[[2]]
boost.Hitters[[5]]
boost.fit1 <- gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000,shrinkage=.0001)
boost.fit2 <- gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000,shrinkage=.0005)
boost.fits <- list(boost.fit1, boost.fit2)
for (i in 3:10){
boost.fit <- gbm(Salary~.,data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
append(boost.fits, boost.fit)
}
boost.fits
for (i in 3:10){
boost.fit <- gbm(Salary~.,data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
boost.fits <- c(boost.fits, boost.fit)
}
boost.fits
boost.fits[1]
boost.fits[2]
boost.fits[3]
boost.fits[4]
for (i in 3:10){
}
boost.fits <- list()
for (i in 1:10){
boost.fit <- gbm(Salary~.,data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
boost.fits <- append(boost.fits, boost.fit)
}
boost.fit[1]
boost.fit <- gbm(Salary~.,data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[1])
boost.fit <- list(boost.fit)
boost.fit <- gbm(Salary~.,data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[1])
boost.fits <- list(boost.fit)
for (i in 2:10){
boost.fit <- gbm(Salary~.,data=training, distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
boost.fits <- append(boost.fits, boost.fit)
}
boost.models <- list()
boost.models[1] <0 gbm(Salary~.data=training,distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkate[1])
boost.models[1] <- gbm(Salary~.data=training,distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkate[1])
boost.models[1] <- gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkate[1])
boost.models[1] <- gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[1])
boost.models[1]
boost.models <- list()
boost.models[[1]] <- gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[1])
boost.models[1]
boost.models <- list()
for (i in 1:10){
boost.models[[i]] <- gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
}
boost.models[3]
boost.models[6]
boost.models[[3]]
boost.models[[3]]
summary(boost.models[[3]])
names(boost.models[[3]])
boost.models[[3]]$train.error
names(boost.models[[3]])
boosts.models[[1:10]]$train.error[1000]
boost.models[[1:10]]$train.error[1000]
boost.models[[1:10]]$train.error
boost.models[[1:10]]
boost.models[1:10]
boost.models[1:10]$train.error[1000]
boost.models[1:10]$train.error
boost.models[[1:10]][]
boost.models[[1:10]][1]
boost.models[[1:10]][1]
boost.models[1:10][]
boost.models[1:10][1]
train.errors <- c()
for(i in 1:10){
train.errors[i] <- boost.models[[i]]$train.error[1000]
}
train.errors
boost.models[10]
boost.models[10]$train.errors[500]
boost.models[10]$train.error[500]
boost.models[10]$train.error
boost.models[10]$train.error
plot(boost.shrinkage,train.errors)
boost.shrinkage <- c(.0001,.001,.01,.1,1,10,.100,1000,10000)
boost.models <- list()
for (i in 1:10){
boost.models[[i]] <- gbm(Salary~.,data=training,distribution="gaussian",n.trees=1000,shrinkage=boost.shrinkage[i])
}
train.errors <- c()
for(i in 1:10){
train.errors[i] <- boost.models[[i]]$train.error[1000]
}
train.errors
boost.models[[i]]
boost.models[[6]]$train.error
